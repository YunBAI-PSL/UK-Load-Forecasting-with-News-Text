import pandas as pdimport numpy as npimport nltkfrom nltk.corpus import stopwordsfrom nltk import word_tokenize, sent_tokenizenltk.download('punkt')nltk.download('stopwords')def count_words(text):    return len(text.split())def count_sentences(text):    return len(sent_tokenize(text))def grouped_to_corpus(grouped, name):    return " ".join(grouped[name].dropna())def count_unique_words(corpus):     return len(set(corpus.split()))def count_non_stopwords(corpus):       stop_words = set(stopwords.words('english'))    word_tokens = word_tokenize(corpus)    return len([w for w in word_tokens if w.lower() not in stop_words])def avg_sentence_length(corpus):    sentences = sent_tokenize(corpus)    return np.mean([count_words(sentence) for sentence in sentences]) if sentences else 0def avg_news_length(grouped, name):    return np.mean([count_sentences(news) for news in grouped[name].dropna()])def count_keyword_occurrences(grouped, name, keyword="electric"):    return grouped[name].str.contains(keyword, case=False, na=False).sum()def get_section(df):    section_counts = df.groupby('section')['title'].count()    main_sections = section_counts[section_counts >= 1000].index.tolist()    return main_sections + ["Others"], set(section_counts.index)def categorize_sections(section_list, section_list_old, df):    df['section'] = df['section'].apply(lambda x: x if x in section_list else 'Others')    return pd.get_dummies(df, columns=['section'], dummy_na=False)def generate_features(df, name, include_sections=False):    df = df.dropna(subset=[name])    df[name + '_word_count'] = df[name].apply(count_words)    df[name + '_sentence_count'] = df[name].apply(count_sentences)    grouped = df.groupby('published_date')        new_df = pd.DataFrame({        'date': grouped.size().index,        'total_words': grouped[name + '_word_count'].sum().values,        'total_sentences': grouped[name + '_sentence_count'].sum().values,        'news_count': grouped.size().values,    })        new_df['unique_words'] = grouped[name].apply(grouped_to_corpus).apply(count_unique_words).values    new_df['non_stopwords'] = grouped[name].apply(grouped_to_corpus).apply(count_non_stopwords).values    new_df['avg_sentence_length'] = grouped[name].apply(grouped_to_corpus).apply(avg_sentence_length).values    new_df['avg_news_length'] = grouped.apply(lambda x: avg_news_length(x, name)).values    new_df['keyword_occurrences'] = grouped.apply(lambda x: count_keyword_occurrences(x, name)).values        if include_sections:        section_list, section_list_old = get_section(df)        df = categorize_sections(section_list, section_list_old, df)        section_df = df.groupby('published_date').mean().reset_index()        new_df = pd.merge(new_df, section_df, on='published_date', how='left')        return new_df# Load datasetdataset = pd.read_excel('bbc_news.xlsx').dropna(subset=['title', 'description', 'content'])print('Data loaded successfully')# Generate featurestitle_features = generate_features(dataset, 'title', include_sections=False)description_features = generate_features(dataset, 'description', include_sections=False)content_features = generate_features(dataset, 'content', include_sections=False)# Save processed featurestitle_features.to_excel('title_features_count.xlsx', index=False)description_features.to_excel('description_features_count.xlsx', index=False)content_features.to_csv('content_features_count.csv', index=False)